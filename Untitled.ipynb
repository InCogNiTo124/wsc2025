{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a45cff2-c1c2-4762-ab32-6657aec1f012",
   "metadata": {},
   "source": [
    "# LLM Serving from the First Principles\n",
    "\n",
    "You can code along the presenter or just run the cells\n",
    "* To move to the next step run `jj next` in the terminal\n",
    "* To move back to the previous step, run `jj prev`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df818f0d-7238-40eb-be4e-bbd4ce8e9c6e",
   "metadata": {},
   "source": [
    "# Step 8: self work\n",
    "\n",
    "We're done with a minimal LLM server! Now it's your turn:\n",
    "\n",
    "- Play with it:\n",
    "  - Try various prompts and see how it influences model output length _and quality_\n",
    "    - https://www.promptingguide.ai/\n",
    "  - Try to feed the entire codebase to the model and see what it recommends to improve next\n",
    "- Refactor:\n",
    "  - Rewrite this whole thing using a [`transformers.pipeline`](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.TextGenerationPipeline)\n",
    "  - Use [FastAPI lifespans](https://fastapi.tiangolo.com/advanced/events/#lifespan-events) to load the model\n",
    "    - You could also run some warmup requests, too\n",
    "  - Dockerize this entire app\n",
    "    - Don't forget to `COPY` the model checkpoints into the container image\n",
    "- New features:\n",
    "  - Remove the prompt prefix from the model output\n",
    "    - easy mode: python string indexing: `decoded_output[index:]`\n",
    "    - hard mode: tensor indexing: `model_output[index:]`\n",
    "    - optionally, find a way to skip special tokens\n",
    "  - Multi-turn conversations\n",
    "    - create new session keys, and store the user prompts and model outputs into a list under the session key\n",
    "  - more metrics\n",
    "    - The metrics were intentionally kept simple. You can also measure tokenization time, decoding time, proportion of all the stages to the total time, number of input tokens, number of tokens that were generated, generated tokens per second, and a myriad of other statistics\n",
    "    - bonus points for figuring out how to read gpu utilization metrics\n",
    "- ... and everything else you come up with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fcdb07-c884-4633-8afc-fcfcb17a88ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device('cuda:0')\n",
    "\n",
    "torch.inference_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602de51-9204-4dbd-a6a9-42481bb26829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GemmaTokenizer, Gemma3ForCausalLM\n",
    "\n",
    "checkpoint = 'google/gemma-3-1b-it'\n",
    "TOKENIZER = GemmaTokenizer.from_pretrained(f'checkpoints/{checkpoint}')\n",
    "MODEL = Gemma3ForCausalLM.from_pretrained(f'checkpoints/{checkpoint}', torch_dtype=torch.bfloat16, device_map=DEVICE).eval()\n",
    "MODEL.generation_config.max_new_tokens = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d1ee98-81ab-4b6b-995a-2125256ca189",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL.generation_config.cache_implementation = 'static'\n",
    "MODEL.forward = torch.compile(MODEL.forward, fullgraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf3c277-58af-4855-bec3-9bb5dd603d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Query(BaseModel):\n",
    "    prompt: str\n",
    "\n",
    "app = FastAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ac9d0-d360-445a-b483-be1b95a05651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "@app.post(\"/infer\")\n",
    "def inference(query: Query):\n",
    "    since = time.time()\n",
    "    tokenized = TOKENIZER.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": query.prompt}],\n",
    "        add_generation_prompt=True,\n",
    "        return_dict=True,\n",
    "        return_tensors='pt').to(DEVICE)\n",
    "    model_outputs = MODEL.generate(**tokenized)\n",
    "    decoded_text = TOKENIZER.decode(model_outputs[0])\n",
    "    latency = time.time() - since\n",
    "    return {\"response\": decoded_text, \"metrics\": {\"latency\": latency, \"tokens_count\": model_outputs.shape[1], \"tokens_per_second\": model_outputs.shape[1]/latency}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d1d50-0584-4d20-9161-d11b9b632c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uvicorn\n",
    "\n",
    "config = uvicorn.Config(app, host=\"0.0.0.0\")\n",
    "server = uvicorn.Server(config)\n",
    "await server.serve()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
