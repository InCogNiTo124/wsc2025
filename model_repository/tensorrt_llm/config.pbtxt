name: "tensorrt_llm"
backend: "tensorrtllm"

model_transaction_policy {
  decoupled: False
}

input {
  name: "input_ids"
  data_type: TYPE_INT32
  dims: [ -1 ]
  reshape: { shape: [ 1, -1 ] }
  allow_ragged_batch: true
}

input {
  name: "request_output_len"
  data_type: TYPE_INT32
  dims: [ 1 ]
}

input {
  name: "end_id"
  data_type: TYPE_INT32
  dims: [ 1 ]
  optional: true
}

output {
  name: "output_ids"
  data_type: TYPE_INT32
  dims: [ 1, 1, -1 ]  # [batch_size, num_outputs, num_tokens]
}
output {
  name: "sequence_length"
  data_type: TYPE_INT32
  dims: [ -1 ]
}

instance_group {
  count: 1
  kind : KIND_GPU
}

parameters: {
  key: "gpt_model_path"
  value {
    string_value: "/wsc2025/checkpoints/trt_out"
  }
}
parameters {
  key: "gpt_model_type"
  value {
    string_value: "inflight_fused_batching"
  }
}

# parameters that always have to be specified due to a bug
parameters {
  key: "tokenizer_dir"
  value {
    string_value: ""
  }
}
parameters {
  key: "xgrammar_tokenizer_info_path"
  value {
    string_value: ""
  }
}
parameters {
  key: "guided_decoding_backend"
  value {
    string_value: ""
  }
}
parameters: {
  key: "kv_cache_free_gpu_mem_fraction"
  value: {
    string_value: "0.5"
  }
}

